{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from syllables import estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(i, url):\n",
    "      \n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title = soup.find('title').text\n",
    "    if i in {0,14,54,55,63,70,71,75}:\n",
    "        content = soup.find_all('div', class_='tdb-block-inner td-fix-index')\n",
    "        content = content[14]\n",
    "    elif i in {7, 20, 107}:\n",
    "        return ''\n",
    "    else:    \n",
    "        content = soup.find('div', class_=\"td-post-content tagdiv-type\")\n",
    "    \n",
    "\n",
    "    result = ''\n",
    "  \n",
    "\n",
    "    if content:\n",
    "        paragraphs = content.find_all('p')  # Use find_all() to get all <p> tags\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_text = paragraph.get_text()  # Using strip=True to remove whitespace strip=True        \n",
    "            result = result + \" \" + paragraph_text\n",
    "\n",
    "    else:\n",
    "        print(\"No article content found.\", i)\n",
    "\n",
    "    return  title + ' ' + result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z]+',' ', text).strip() \n",
    "    tokens = word_tokenize(text)    \n",
    "\n",
    "    #lemmatize and remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the positive and negative word lists\n",
    "with open('positive-words.txt', 'r') as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "\n",
    "with open('negative-words.txt', 'r') as f:\n",
    "    negative_words = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg(idx, url, tokens):\n",
    "        \n",
    "    # Count positive and negative words\n",
    "    positive_count = sum(1 for word in tokens if lemmatizer.lemmatize(word) in positive_words)\n",
    "    negative_count = sum(1 for word in tokens if lemmatizer.lemmatize(word) in negative_words)\n",
    "    #polarity score\n",
    "    polarity_score =  (positive_count - negative_count) / ((positive_count + negative_count) + 0.000001)\n",
    "    #subjective score\n",
    "    total_words = len(tokens)\n",
    "    subjective_score = (positive_count + negative_count)/ ((total_words) + 0.000001)\n",
    "    # average sentence length\n",
    "    number_of_sentences = sent_tokenize(get_text(idx, url))\n",
    "    number_of_sentence = len(number_of_sentences) - 1 \n",
    "    avg_sent_len =  total_words / number_of_sentence  \n",
    "    #complex words\n",
    "    complex_words = len([word for word in tokens if estimate(word) > 2])\n",
    "    percentage_complex = complex_words / (total_words + 0.0001)\n",
    "    percentage_complex\n",
    "    #fog index\n",
    "    fog_index = 0.4 * (avg_sent_len + percentage_complex)\n",
    "    #personal pronouns\n",
    "    personal_pronouns = {\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"myself\", \"yourself\", \"himself\", \"herself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\"}\n",
    "    personal_pronoun_count = sum(1 for word in tokens if word in personal_pronouns)\n",
    "    #avg word len\n",
    "    total_characters = sum(len(word) for word in tokens)\n",
    "    total_words = len(tokens)\n",
    "    average_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "\n",
    "    return [positive_count, negative_count, polarity_score, subjective_score, avg_sent_len, percentage_complex, fog_index, avg_sent_len, complex_words, total_words, complex_words, personal_pronoun_count, average_word_length] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in df.index:\n",
    "    url = df['URL'][idx]\n",
    "    text = get_text(idx, url)\n",
    "    tokens = preprocess(text)\n",
    "    data = pos_neg(idx, url, tokens)\n",
    "    df.iloc[idx : idx + 1, 2:] = data\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
